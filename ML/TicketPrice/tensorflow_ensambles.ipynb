{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\DataSci\\.mlws\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.layers import Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Alex\\.cache\\kagglehub\\datasets\\ibrahimelsayed182\\plane-ticket-price\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (example: historical ticket pricing data)\n",
    "path = kagglehub.dataset_download(\"ibrahimelsayed182/plane-ticket-price\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "data_path = fr'{path}\\ticket_pricing_data.csv'\n",
    "data = pd.read_csv(r'C:\\Users\\Alex\\.cache\\kagglehub\\datasets\\ibrahimelsayed182\\plane-ticket-price\\versions\\1\\Data_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Date_of_Journey</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Route</th>\n",
       "      <th>Dep_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "      <th>Journey_day</th>\n",
       "      <th>Journey_month</th>\n",
       "      <th>Dep_hour</th>\n",
       "      <th>Dep_minute</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Duration_Minutes</th>\n",
       "      <th>Arrival_hour</th>\n",
       "      <th>Arrival_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1900-01-01 22:20:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>8</td>\n",
       "      <td>3897</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>1900-01-01 05:50:00</td>\n",
       "      <td>1900-01-01 13:15:00</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>8</td>\n",
       "      <td>7662</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>445</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>1900-01-01 09:25:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>8</td>\n",
       "      <td>13882</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-05-12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>1900-01-01 18:05:00</td>\n",
       "      <td>1900-01-01 23:30:00</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>8</td>\n",
       "      <td>6218</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>325</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>1900-01-01 16:50:00</td>\n",
       "      <td>1900-01-01 21:35:00</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>8</td>\n",
       "      <td>13302</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>285</td>\n",
       "      <td>21.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Airline Date_of_Journey  Source  Destination  Route            Dep_Time  \\\n",
       "0        3      2019-03-24       0            5     18 1900-01-01 22:20:00   \n",
       "1        1      2019-05-01       3            0     84 1900-01-01 05:50:00   \n",
       "2        4      2019-06-09       2            1    118 1900-01-01 09:25:00   \n",
       "3        3      2019-05-12       3            0     91 1900-01-01 18:05:00   \n",
       "4        3      2019-03-01       0            5     29 1900-01-01 16:50:00   \n",
       "\n",
       "         Arrival_Time Total_Stops  Additional_Info  Price  Journey_day  \\\n",
       "0                 NaT    non-stop                8   3897           24   \n",
       "1 1900-01-01 13:15:00     2 stops                8   7662            1   \n",
       "2                 NaT     2 stops                8  13882            9   \n",
       "3 1900-01-01 23:30:00      1 stop                8   6218           12   \n",
       "4 1900-01-01 21:35:00      1 stop                8  13302            1   \n",
       "\n",
       "   Journey_month  Dep_hour  Dep_minute  Day_of_Week  Month  Day  \\\n",
       "0              3        22          20            6      3   24   \n",
       "1              5         5          50            2      5    1   \n",
       "2              6         9          25            6      6    9   \n",
       "3              5        18           5            6      5   12   \n",
       "4              3        16          50            4      3    1   \n",
       "\n",
       "   Duration_Minutes  Arrival_hour  Arrival_minute  \n",
       "0               170           NaN             NaN  \n",
       "1               445          13.0            15.0  \n",
       "2              1140           NaN             NaN  \n",
       "3               325          23.0            30.0  \n",
       "4               285          21.0            35.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: There are NaN values in the 'Total_Stops' column after mapping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocessing Date and Time columns\n",
    "data['Date_of_Journey'] = pd.to_datetime(data['Date_of_Journey'], format='%d/%m/%Y')\n",
    "data['Journey_day'] = data['Date_of_Journey'].dt.day\n",
    "data['Journey_month'] = data['Date_of_Journey'].dt.month\n",
    "\n",
    "# Function to clean and convert time\n",
    "def clean_and_convert_time(time_str):\n",
    "    if isinstance(time_str, str) and ' ' in time_str:  # Check if it's a string and contains a space\n",
    "        return None  # Or assign a default time like '00:00'\n",
    "    try:\n",
    "        return pd.to_datetime(time_str, format='%H:%M', errors='coerce')\n",
    "    except Exception:\n",
    "        return None  # If conversion fails, return None\n",
    "\n",
    "data['Dep_Time'] = data['Dep_Time'].apply(clean_and_convert_time)\n",
    "data['Dep_hour'] = data['Dep_Time'].dt.hour\n",
    "data['Dep_minute'] = data['Dep_Time'].dt.minute\n",
    "\n",
    "data['Arrival_Time'] = data['Arrival_Time'].apply(clean_and_convert_time)\n",
    "data['Arrival_hour'] = data['Arrival_Time'].dt.hour\n",
    "data['Arrival_minute'] = data['Arrival_Time'].dt.minute\n",
    "\n",
    "# Handle categorical features\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Airline', 'Source', 'Destination', 'Route', 'Additional_Info']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le  # Save label encoder for possible inverse transformation later\n",
    "\n",
    "# No need to parse 'Duration_Minutes' as it's already an integer\n",
    "# Just make sure it's in the right format\n",
    "if 'Duration_Minutes' in data.columns:\n",
    "    data['Duration'] = data['Duration_Minutes']\n",
    "else:\n",
    "    print(\"Warning: 'Duration_Minutes' column not found in the DataFrame\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(['Price', 'Date_of_Journey', 'Dep_Time', 'Arrival_Time'], axis=1)\n",
    "y = data['Price']\n",
    "\n",
    "# Normalize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X[['Journey_day', 'Journey_month', 'Dep_hour', 'Dep_minute', 'Arrival_hour', 'Arrival_minute', 'Duration']] = scaler.fit_transform(\n",
    "    X[['Journey_day', 'Journey_month', 'Dep_hour', 'Dep_minute', 'Arrival_hour', 'Arrival_minute', 'Duration']]\n",
    ")\n",
    "\n",
    "# Define a mapping dictionary for Total_Stops\n",
    "stops_mapping = {\n",
    "    'non-stop': 0,\n",
    "    '1 stop': 1,\n",
    "    '2 stops': 2,\n",
    "    '3 stops': 3,\n",
    "    '4 stops': 4\n",
    "}\n",
    "\n",
    "# Apply the mapping to the Total_Stops column\n",
    "data['Total_Stops'] = data['Total_Stops'].map(stops_mapping)\n",
    "\n",
    "# Check if there are any NaN values after mapping\n",
    "if data['Total_Stops'].isnull().any():\n",
    "    print(\"Warning: There are NaN values in the 'Total_Stops' column after mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_1(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output price\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output price\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Reshape((input_shape[0], 1), input_shape=input_shape),\n",
    "        layers.Conv1D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(128, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)  # Output price\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\DataSci\\.mlws\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Projects\\DataSci\\.mlws\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9099.1279 - val_loss: 9135.4531\n",
      "Epoch 2/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9052.8320 - val_loss: 9134.7041\n",
      "Epoch 3/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9020.8164 - val_loss: 9133.4492\n",
      "Epoch 4/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9023.2715 - val_loss: 9131.4189\n",
      "Epoch 5/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9081.6074 - val_loss: 9128.4219\n",
      "Epoch 6/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9188.1357 - val_loss: 9124.3721\n",
      "Epoch 7/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9050.3848 - val_loss: 9119.2344\n",
      "Epoch 8/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9034.1221 - val_loss: 9113.0107\n",
      "Epoch 9/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9093.5273 - val_loss: 9105.7002\n",
      "Epoch 10/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8997.2510 - val_loss: 9097.3223\n",
      "Training Model 2...\n",
      "Epoch 1/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9029.5752 - val_loss: 9134.4834\n",
      "Epoch 2/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9070.8301 - val_loss: 9124.8223\n",
      "Epoch 3/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9091.8818 - val_loss: 9079.4453\n",
      "Epoch 4/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9038.2227 - val_loss: 8959.6416\n",
      "Epoch 5/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8787.3115 - val_loss: 8735.4668\n",
      "Epoch 6/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8598.5615 - val_loss: 8386.7178\n",
      "Epoch 7/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8217.9561 - val_loss: 7897.8887\n",
      "Epoch 8/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7734.6768 - val_loss: 7255.8584\n",
      "Epoch 9/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6918.7188 - val_loss: 6481.4551\n",
      "Epoch 10/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6196.8096 - val_loss: 5605.1084\n",
      "Training Model 3...\n",
      "Epoch 1/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9124.5488 - val_loss: 9135.8623\n",
      "Epoch 2/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9072.1279 - val_loss: 9135.7295\n",
      "Epoch 3/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9088.4053 - val_loss: 9135.5928\n",
      "Epoch 4/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9028.3027 - val_loss: 9135.4648\n",
      "Epoch 5/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9005.9785 - val_loss: 9135.3291\n",
      "Epoch 6/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9122.3770 - val_loss: 9135.1953\n",
      "Epoch 7/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9163.8242 - val_loss: 9135.0596\n",
      "Epoch 8/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9059.4561 - val_loss: 9134.9258\n",
      "Epoch 9/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9031.8057 - val_loss: 9134.7920\n",
      "Epoch 10/10\n",
      "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9092.7393 - val_loss: 9134.6553\n"
     ]
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1],)  # Shape of the input features\n",
    "\n",
    "model1 = create_model_1(input_shape)\n",
    "model2 = create_model_2(input_shape)\n",
    "model3 = create_model_3(input_shape)\n",
    "\n",
    "# Train the models\n",
    "models_list = [model1, model2, model3]\n",
    "\n",
    "for i, model in enumerate(models_list):\n",
    "    print(f\"Training Model {i+1}...\")\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7994.3066 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7919.904296875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def averaging_ensemble(models, input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    outputs = [model(inputs) for model in models]\n",
    "    avg_output = tf.keras.layers.Average()(outputs)\n",
    "    ensemble_model = tf.keras.Model(inputs=inputs, outputs=avg_output)\n",
    "    return ensemble_model\n",
    "\n",
    "ensemble_avg = averaging_ensemble(models_list, input_shape)\n",
    "ensemble_avg.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_avg.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8156.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8081.72216796875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def stacking_ensemble(models, input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    outputs = [model(inputs) for model in models]\n",
    "    concatenated = Concatenate()(outputs)\n",
    "    \n",
    "    meta_output = Dense(1)(concatenated)\n",
    "    stacked_model = Model(inputs=inputs, outputs=meta_output)\n",
    "    return stacked_model\n",
    "\n",
    "ensemble_stack = stacking_ensemble(models_list, input_shape)\n",
    "ensemble_stack.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Evaluate stacking model\n",
    "ensemble_stack.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "MAE of Averaging Ensemble: 7919.908203125\n",
      "MAE of Stacking Ensemble: 8081.7255859375\n"
     ]
    }
   ],
   "source": [
    "# Predictions for ensemble models\n",
    "y_pred_avg = ensemble_avg.predict(X_test)\n",
    "y_pred_stack = ensemble_stack.predict(X_test)\n",
    "\n",
    "# Calculate MAE for both models\n",
    "mae_avg = mean_absolute_error(y_test, y_pred_avg)\n",
    "mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"MAE of Averaging Ensemble: {mae_avg}\")\n",
    "print(f\"MAE of Stacking Ensemble: {mae_stack}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
